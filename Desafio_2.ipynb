{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW1oaTqwk5H9hc9SAS+Pva",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ematevez/76195-Inteligencia-Artificial/blob/main/Desafio_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DESAFIO 2:\n",
        "\n",
        "Escribe un script en Python que utilice el modelo GPT4All para generar tareas pendientes en formato JSON a partir de una descripción proporcionada por el usuario.\n",
        "\n",
        "Requisitos:\n",
        "\n",
        "Importar los módulos json y GPT4All.\n",
        "Cargar el modelo Meta-Llama-3-8B-Instruct.Q4_0.gguf.\n",
        "Definir una función generar_tareas_json(prompt) que use una sesión de chat del modelo para generar una respuesta con un límite de 512 tokens.\n",
        "Definir una función procesar_tarea_json(respuesta_json) que extraiga información clave del JSON generado, como \"título\", \"descripción\", \"fecha\" y \"prioridad\". Si la respuesta no es un JSON válido, debe manejar el error y mostrar la respuesta generada.\n",
        "Solicitar al usuario una descripción de la tarea mediante input().\n",
        "Construir un prompt que le indique al modelo que genere una tarea en formato JSON con los campos mencionados.\n",
        "Mostrar la respuesta generada y procesarla para extraer la información estructurada.\n",
        "Devuelve el código completo en Python."
      ],
      "metadata": {
        "id": "G0NElhciV07w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar la biblioteca gpt4all\n",
        "!pip install gpt4all\n",
        "\n",
        "# Descargar un modelo compatible\n",
        "!wget https://gpt4all.io/models/Meta-Llama-3-8B-Instruct.Q4_0.gguf -O Meta-Llama-3-8B-Instruct.Q4_0.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "121S-vwaVoVP",
        "outputId": "98e17acc-2f16-4274-dff7-5f8b8b66e30d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpt4all\n",
            "  Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from gpt4all) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gpt4all) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->gpt4all) (2024.12.14)\n",
            "Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gpt4all\n",
            "Successfully installed gpt4all-2.8.2\n",
            "--2025-01-29 23:13:03--  https://gpt4all.io/models/Meta-Llama-3-8B-Instruct.Q4_0.gguf\n",
            "Resolving gpt4all.io (gpt4all.io)... 172.67.71.169, 104.26.1.159, 104.26.0.159, ...\n",
            "Connecting to gpt4all.io (gpt4all.io)|172.67.71.169|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-01-29 23:13:03 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srsMlHU4Vkhs",
        "outputId": "4834bd08-3d68-4b8b-a49b-324dddcc2909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 4.66G/4.66G [01:42<00:00, 45.6MiB/s]\n",
            "Verifying: 100%|██████████| 4.66G/4.66G [00:16<00:00, 291MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ingresa la descripción de la tarea: hacer una torta\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from gpt4all import GPT4All\n",
        "\n",
        "# Cargar el modelo de GPT4All\n",
        "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\")\n",
        "\n",
        "# Función para generar tareas en formato JSON\n",
        "def generar_tareas_json(prompt):\n",
        "    \"\"\"\n",
        "    Genera una tarea pendiente en formato JSON utilizando el modelo GPT4All.\n",
        "    \"\"\"\n",
        "    with model.chat_session():\n",
        "        respuesta = model.generate(prompt, max_tokens=512, )\n",
        "    return respuesta\n",
        "\n",
        "# Función para limpiar y procesar la respuesta JSON\n",
        "def procesar_tarea_json(respuesta_json):\n",
        "    \"\"\"\n",
        "    Procesa una respuesta en formato JSON y extrae información clave.\n",
        "    Intenta limpiar el contenido si no es un JSON válido.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Buscar el contenido que parece un JSON\n",
        "        inicio = respuesta_json.find(\"{\")\n",
        "        fin = respuesta_json.rfind(\"}\") + 1\n",
        "        json_limpio = respuesta_json[inicio:fin]\n",
        "\n",
        "        # Convertir el contenido a un diccionario\n",
        "        tarea = json.loads(json_limpio)  # Convertir de JSON a diccionario\n",
        "        titulo = tarea.get(\"titulo\", \"Título no especificado\")\n",
        "        descripcion = tarea.get(\"descripcion\", \"Descripción no especificada\")\n",
        "        fecha = tarea.get(\"fecha\", \"Fecha no especificada\")\n",
        "        prioridad = tarea.get(\"prioridad\", \"Prioridad no especificada\")\n",
        "\n",
        "        # Imprimir los datos extraídos\n",
        "        print(\"\\nInformación clave extraída:\")\n",
        "        print(f\"Título: {titulo}\")\n",
        "        print(f\"Descripción: {descripcion}\")\n",
        "        print(f\"Fecha: {fecha}\")\n",
        "        print(f\"Prioridad: {prioridad}\")\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error: La respuesta no es un JSON válido.\")\n",
        "        print(f\"Detalles del error: {e}\")\n",
        "        print(\"\\nRespuesta completa generada:\")\n",
        "        print(respuesta_json)\n",
        "\n",
        "# Solicitar al usuario la descripción de la tarea\n",
        "descripcion_usuario = input(\"Ingresa la descripción de la tarea: \")\n",
        "\n",
        "# Crear el prompt dinámico a partir de la descripción ingresada\n",
        "prompt_json = f\"\"\"\n",
        "Genera una tarea pendiente para una reunión de equipo en formato JSON.\n",
        "El formato esperado es el siguiente:\n",
        "{{\n",
        "    \"titulo\": \"string\",\n",
        "    \"descripcion\": \"string\",\n",
        "    \"fecha\": \"DD-MM-AAAA\",\n",
        "    \"prioridad\": \"1-Alta/2-Media/3-Baja\"\n",
        "}}\n",
        "Ejemplo:\n",
        "{{\n",
        "    \"titulo\": \"Revisión de métricas anuales\",\n",
        "    \"descripcion\": \"Analizar las métricas del año para planificar objetivos del próximo año.\",\n",
        "    \"fecha\": \"2024-12-15\",\n",
        "    \"prioridad\": \"Alta\"\n",
        "}}\n",
        "Ahora genera la tarea basada en la siguiente descripción:\n",
        "\"{descripcion_usuario}\"\n",
        "\"\"\"\n",
        "\n",
        "# Generar la tarea en formato JSON\n",
        "respuesta_generada = generar_tareas_json(prompt_json)\n",
        "print(\"\\nRespuesta generada (JSON):\")\n",
        "print(respuesta_generada)\n",
        "\n",
        "# Procesar la respuesta JSON\n",
        "procesar_tarea_json(respuesta_generada)"
      ]
    }
  ]
}